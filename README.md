# desafio-tecnico-ga_intergado
Esse projeto faz parte do processo seletivo para a vaga de Engenheiro/Analista de Dados da empresa GA + Intergado. O objetivo do teste técnico foi utilizar dados IPCA extraídos da API do Banco Central do Brasil (BCB) para atualizar um dataset fornecidos com dados atualizados e fazer um upsert na base de destino, carregando os dados e salvando-os no formato ".parquet". Como opcional, foi realizada a implementação desse processo ETL dentro de uma dag do Airflow.

O projeto resolvido consta com um jupyter notebook intitulado "jupyter-notebook-desafio-tecnico-wellington.ipynb", que mostra o desenvolvimento da solução ETL bem como todas as saídas de cada etapa. Também há um arquivo com as instruções do desafio passadas pela empresa e um arquivo texto intitulado "docker-container-run.txt", que apresenta instruções para rodar o container do Airflow, passando a porta 8080 para conexão com a porta 8080 do Airflow e acessar via browser.

A dag, que está dentro da pasta "dags", foi construída pensando em explorar diferentes recursos do Airflow. Ela foi dividida em tarefas tentando preservar o Escopo do teste técnico, utilizando o XCom para passar o diretório dos arquivos intermediários entre uma tarefa e outra. Arquivos ".parquet" intermediários foram utilizados para passar os dataframes entre uma tarefa e outra, pensando no modelo do teste técnico e entendendo que as bases de dados fornecidas são pequenas. Em uma situação real, provavelmente esse processo de "writes e reads" em cada tarefa seria inviável, e o ideal passaria a ser condensar o número de tarefas para trabalhar com os dataframes de uma única vez e adicionar logs intermediários entre cada processo ETL.

Ademais, outras recomendações para um processo de maior escala seria criar um módulo separados para as funções e importá-las para a dag, deixando-a mais limpa e legível, e adicionar mais try/except em pontos de possíveis falhas em um cenário de produção.